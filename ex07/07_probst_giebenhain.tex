\documentclass{article}
\usepackage[en]{ukon-infie}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{graphicx}
% kann de oder en sein
% kann bubble break, topexercise sein

\Names{Jonas Probst, Simon Giebenhain}
\Lecture[AnaVis]{Analyse und Visualisierung von Informationen}
\Term{WS 2017/18}

\begin{document}
    \begin{ukon-infie}[12.12.17]{7}

        \begin{exercise}[p=7]{Single/Average Linkage}  
        \question{}
        {
        	The distance matrix:\\
        	\includegraphics[scale=0.5]{distance_matrix.png}\\
        	
        	We came up with the followingdendrogram. \\
        	\includegraphics[scale=0.6]{cluster_tree.png}
        }
    	\question{}
    	{
    		\includegraphics[scale=0.6]{avg_cluster_tree_continued.png}	
    	}

		\end{exercise}
		
		\begin{exercise}[p=2]{DENCLUE}
		$\xi=0$\\
		Center defined clusters.\\
		Influence function: Gaussian function, with $\sigma$ so that there are $k$ clusters.\\
		\end{exercise}
		
		\begin{exercise}[p=7]{EM}
		
		\question{}
		{
			EM assumes, that every cluster can be described by a d-dimensional gaussian distribution (d is dimension of data). Each point is assigned to each cluster, with a probabilty given by the Gaussian distribution of the cluster. From these probabilties the clusters (the Gaussian distributions representing the clusters) are recalculated. This process is repeated until the gain of the expected value is below some percision $\epsilon$.
		}
		
		\question{}
		{
			EM treats all points in the same way. Noise and outliers dont play a special role.\\
			However outliers and noise contirbute to the clusters, therfore influencing their mean values and variances. This means that noise/outliers can distort the gaussian distributions. They can shift the mean value from the actual cluster center and strecht the variance longer than of the actual cluster.
		}
		\question{}
		{}
After plotting the data we know that there are 4 clusters so we set that parameter to 4. The initalization method is Rnd.EM to make a random initalization.
\begin{verbatim}
library(EMCluster)
library(readr)
em_example <- read_csv("Documents/AnaVis/AnaVis/ex07/em-example.csv")
ret.em <- init.EM(em_example, nclass = 4, method = "Rnd.EM")
plotem(ret.em, em_example, main = "em")
\end{verbatim}
\includegraphics[scale=0.5]{EMplot.png}
\end{exercise}

		
		\begin{exercise}[p=3]{Noise}
		
			\textbf{K-Means}:\\
			K-Means does not treat noise seperately. Each point (including noise) is assigned to exactly one cluster.\\
				\begin{itemize}
				 \item \textbf{Uniform noise:} Depends on the ratio of noise. If there is significantly less noise than actual data points, the effect of the noise will be small. However with more noise, the cluster means will be significantly shifted by the influence of the noise, thereby deteriorating the quality of the clustering. Because the noise is uniformly distributed and affects all clusters similarily the effect wont be large.
				 \item \textbf{Non-uniform noise:} Blops of noise with many points might constitue an own cluster. If the noise is unevenly split among the clusters (, which is much more likley with a non-uniform distribution,) the mean values of the clusters can be significantly shifted and destroy the clustering.
				\end{itemize}
				
				
				\bigskip
						
			\textbf{Single-linkage}:\\
			Noise is indirectly classified, depending on the cutting level of the hierarchical clusters. The points, which have not been merged until the cutting point, constitute the noise cluster. Noise, which is in direct proximity of a cluster, will always be added to that cluster.
			\begin{itemize}
				 \item \textbf{Uniform noise:} If the noise is denser than some clusters, these clusters will vanish in the noise. Otherwise the noise is no problem.
				 \item \textbf{Non-uniform noise:} If there are some places with dense noise, they will be identified as a cluster. Furthermore even a very narrow chain (but relatively dense) can connect real clusters together.
			\end{itemize}
			
			\textbf{DBSCAN}:\\
			Less dense areas (depending on the parameters $\epsilon$ and minPoints) will be classified as noise.\\
			\begin{itemize}
				 \item \textbf{Uniform noise:} If the uniform noise is too dense for $\epsilon$ and minPoints, then everything will be identified as a single cluster. If the noise is less dense, no noise point will be classified as a core point, therefore most of the noise will be classified as noise. However some noise points might be recognized as boarder points of real clusters.
				 \item \textbf{Non-uniform noise:} Dense noise will be identified as clusters. However less dense noise will always be classified as such.
			\end{itemize}
			
			
		\end{exercise}
		
		\begin{exercise}[p=7]{Unknown Data}
			First and foremost one has to normalize the dataset before even considering to cluster the data. This is important because some columns cointain values in different scales. The \textbf{scale()} function is a handy function, which normalizes the data in such a way, that the mean is zero and has a standar deviation of 1.\\
			Afterwards we removed points, which seemed to be noise with the method of wishart.\\
			Then we applied a hierachical clustering, using ward's method as a measure for the hierarchical clustering. We recieved the following tree strucutre.\\
			\includegraphics[scale=0.4]{hclust_tree.png}\\
			From the level of 3 clusters, there is a huge distance gap to less clusters. This is an indicator, that this is a nice point to cut the tree.\\
			In order to evaluate our results, we performed a PCA to reduce the dimensions of the data. Thus we were able to reduce the data to 3 dimension, which explain most of the data. In the transformed space, we visalized our clusters.\\
			Without removing the noise, this yielded the following:\\
			\includegraphics[scale=0.6]{pca_cluster_with_noise.png}\\
			With removing some noise befor the clustering we got the following results:\\
			\includegraphics[scale=0.6]{pca_cluster_without_noise.png}
			
			\begin{verbatim}
			library(readr)
			dataset <- read_csv("Downloads/dataset.csv", col_names = FALSE)
			data <- dataset[, 2:14]
			data_norm <- scale(data)
			minPoints <- 3
			epsilon <- 3
			noise <- c()
			for (i in 1:nrow(data_norm)) {
			    counter <- 0
			    for (j in 1:nrow(data_norm)) {
			        if (dist(rbind(data_norm[i, ], data_norm[j, ])) < epsilon) {
			            counter <- counter + 1
			        }
			    }
			    if (counter < minPoints) {
			        noise[length(noise) + 1] <- i
			    }
			}
			data_norm <- data_norm[-noise, ]
			c_ward <- hclust(dist(data_norm), method = "ward.D2")
			ward_cut <- cutree(c_ward, 3)
			pca <- prcomp(data[], center=TRUE, scale.=TRUE)
			dim_red <- predict(pca, newdata = data[])
			col <- c()
			j <- 1
			for (i in 1:178) {
			    if (i %in% noise) {
			        col[i] <- 0
			    } else {
			       col[i] <- ward_cut[j]
			        j <- j + 1
			    }
			}
			library('plotly')
			plot_ly(data.frame(dim_red), x = dim_red[, 1], y = dim_red[, 2], z=dim_red[, 3], color = col)
			
			
			\end{verbatim}
		\end{exercise}
		
		
\end{ukon-infie}
\end{document}
